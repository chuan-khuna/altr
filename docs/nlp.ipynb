{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6689a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pymonad.tools import curry\n",
    "\n",
    "import gensim\n",
    "\n",
    "\n",
    "from altr.nlp import compose, exclude_by_regex, prepare_data_for_ngram, process_ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a4fd9e",
   "metadata": {},
   "source": [
    "load example texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "064fca43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':3', 'â˜ºï¸', 'ðŸ¤¤', 'ðŸ¤ª', 'ðŸ˜', 'ðŸ˜„', 'ðŸ˜Š', 'ðŸ˜‹', 'ðŸ˜', 'ðŸ˜˜']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"https://raw.githubusercontent.com/PyThaiNLP/wisesight-sentiment/refs/heads/master/pos.txt\")\n",
    "\n",
    "texts = [text.strip() for text in response.text.split(\"\\n\")]\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0186e8ce",
   "metadata": {},
   "source": [
    "define tokeniser function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8971a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(texts):\n",
    "    return [word_tokenize(text, engine=\"newmm\", keep_whitespace=False) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e2b25e",
   "metadata": {},
   "source": [
    "define some functions for text and token processing pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41253543",
   "metadata": {},
   "outputs": [],
   "source": [
    "@curry(2)\n",
    "def filter_by_length(length: int, texts: list[str]) -> list[str]:\n",
    "    return [text for text in texts if len(text) >= length]\n",
    "\n",
    "\n",
    "@curry(2)\n",
    "def exclude_tokens_by_regex(pattern: str, list_of_tokens: list[list[str]]) -> list[list[str]]:\n",
    "    return [exclude_by_regex(pattern)(tokens) for tokens in list_of_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667e560c",
   "metadata": {},
   "source": [
    "define functions for process ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b77b495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@curry(2)\n",
    "def train_ngram_model(kwargs: dict, tokenised_texts: list[list[str]]) -> gensim.models.Phrases:\n",
    "    return gensim.models.Phrases(tokenised_texts, **kwargs)\n",
    "\n",
    "\n",
    "def apply_ngram_model(ngram_model: gensim.models.Phrases, tokenised_texts: list[list[str]]) -> list[list[str]]:\n",
    "    return [ngram_model[tokens] for tokens in tokenised_texts]\n",
    "\n",
    "\n",
    "@curry(2)\n",
    "def filter_only_ngram_tokens(delimiter, tokenised_texts: list[list[str]]) -> list[list[str]]:\n",
    "    return [[token for token in tokens if delimiter in token] for tokens in tokenised_texts]\n",
    "\n",
    "\n",
    "@curry(2)\n",
    "def concat_ngram_tokens(delimiter, tokenised_texts: list[list[str]]) -> list[list[str]]:\n",
    "    return [[token.replace(delimiter, \"\") for token in tokens] for tokens in tokenised_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f6625f",
   "metadata": {},
   "source": [
    "create pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab6d38cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training ngram pipeline\n",
    "\n",
    "NGRAM_DELIMITER = \"<DELIM>\"\n",
    "\n",
    "process_bigram = process_ngram(\n",
    "    train_ngram_model({\"min_count\": 1, \"threshold\": 0.1, \"delimiter\": NGRAM_DELIMITER}),\n",
    "    apply_ngram_model,\n",
    "    filter_only_ngram_tokens(NGRAM_DELIMITER),\n",
    "    concat_ngram_tokens(NGRAM_DELIMITER),\n",
    ")\n",
    "\n",
    "process_trigram = process_ngram(\n",
    "    train_ngram_model({\"min_count\": 1, \"threshold\": 0.1, \"delimiter\": NGRAM_DELIMITER}),\n",
    "    apply_ngram_model,\n",
    "    filter_only_ngram_tokens(NGRAM_DELIMITER),\n",
    "    concat_ngram_tokens(NGRAM_DELIMITER),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cce0c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final text processing pipeline\n",
    "\n",
    "text_processing_pipeline = compose(\n",
    "    # filter long enough texts\n",
    "    filter_by_length(20),\n",
    "    tokenise,\n",
    "    exclude_tokens_by_regex(r\"^5\"),\n",
    "    exclude_tokens_by_regex(r\"^\\s+$\"),\n",
    "    # --- train ngram models below ---\n",
    "    prepare_data_for_ngram,\n",
    "    process_bigram,\n",
    "    process_trigram,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bedd3e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = text_processing_pipeline(texts)\n",
    "models, ngrams, ngrams_filtered = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e368463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['à¸à¸´à¸™à¸™à¹‰à¸³à¸‹à¸¸à¸›', 'à¸™à¸°à¸­à¸£à¹ˆà¸­à¸¢'],\n",
       " ['à¸™à¸°à¸¡à¸¶à¸‡'],\n",
       " [],\n",
       " ['à¸„à¸´à¸”à¸–à¸¶à¸‡à¹à¸¡à¹ˆ'],\n",
       " ['à¹€à¸„à¸£', 'à¸–à¹‰à¸²à¹„à¸¡à¹ˆ'],\n",
       " ['à¹ƒà¸„à¸£à¸§à¹ˆà¸²', 'à¸ˆà¸°à¹€à¸¥à¸µà¹‰à¸¢à¸‡'],\n",
       " ['à¸Šà¹ˆà¸§à¸‡à¸™à¸µà¹‰à¸ˆà¸°', 'à¸à¸£à¸­à¸šà¹†'],\n",
       " ['à¸Šà¹ˆà¸§à¸¢à¹†', 'à¸”à¹‰à¸§à¸¢à¸™à¸°'],\n",
       " ['à¸Šà¸­à¸šà¸à¸´à¸™', 'à¸Šà¹‰à¸²à¸‡à¸„à¸£à¸±à¸š', 'à¸žà¸µà¹ˆà¸™à¹‰à¸³'],\n",
       " ['à¹€à¸¡à¸™à¸¹à¸‚à¸­à¸‡']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigram from the first 10 texts\n",
    "ngrams_filtered[2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c1fdd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['à¹à¸–à¸§à¸™à¸µà¹‰à¸¡à¸µ'],\n",
       " ['à¸„à¹ˆà¸²à¸²à¸²^^'],\n",
       " ['à¹à¸›à¹‰à¸‡à¹€à¸ˆà¹‰à¸²à¸™à¸²à¸‡'],\n",
       " ['à¸§à¸±à¸™à¸™à¸µà¹‰à¸›à¹ˆà¸°à¸¥à¹ˆà¸°'],\n",
       " ['à¸¢à¸¹à¹€à¸‹à¸­à¸£à¸µ'],\n",
       " ['à¹„à¸›à¸à¸´à¸™à¸šà¸²à¸šà¸µà¸à¹‰à¸­à¸™'],\n",
       " ['à¸ªà¸¹à¹‰à¹†à¸„à¸±à¸š'],\n",
       " ['à¸­à¸¢à¸²à¸à¸à¸´à¸™à¸šà¸²à¸šà¸´'],\n",
       " ['à¸­à¸¢à¸²à¸à¸à¸´à¸™à¹€à¸­à¸¡à¹€à¸„'],\n",
       " ['à¹„à¸›à¸à¸´à¸™à¸«à¸™à¹ˆà¸­à¸¢']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trigram from the first 10 texts\n",
    "list(filter(lambda tokens: len(tokens) > 0, ngrams_filtered[3]))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a98c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
