{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6689a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pymonad.tools import curry\n",
    "\n",
    "import gensim\n",
    "\n",
    "\n",
    "from altr.nlp import compose, exclude_by_regex, prepare_data_for_ngram, process_ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a4fd9e",
   "metadata": {},
   "source": [
    "load example texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "064fca43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':3', '‚ò∫Ô∏è', 'ü§§', 'ü§™', 'üòÅ', 'üòÑ', 'üòä', 'üòã', 'üòç', 'üòò']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"https://raw.githubusercontent.com/PyThaiNLP/wisesight-sentiment/refs/heads/master/pos.txt\")\n",
    "\n",
    "texts = [text.strip() for text in response.text.split(\"\\n\")]\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0186e8ce",
   "metadata": {},
   "source": [
    "define tokeniser function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8971a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(texts):\n",
    "    return [word_tokenize(text, engine=\"newmm\", keep_whitespace=False) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e2b25e",
   "metadata": {},
   "source": [
    "define some functions for text and token processing pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41253543",
   "metadata": {},
   "outputs": [],
   "source": [
    "@curry(2)\n",
    "def filter_by_length(length: int, texts: list[str]) -> list[str]:\n",
    "    return [text for text in texts if len(text) >= length]\n",
    "\n",
    "\n",
    "@curry(2)\n",
    "def filter_list_tokens_by_regex(pattern: str, list_of_tokens: list[list[str]]) -> list[list[str]]:\n",
    "    return [exclude_by_regex(pattern)(tokens) for tokens in list_of_tokens]\n",
    "\n",
    "\n",
    "NGRAM_DELIMITER = \"<DELIMITER>\"\n",
    "\n",
    "\n",
    "@curry(2)\n",
    "def train_ngram_model(kwargs: dict, tokenised_texts: list[list[str]]) -> gensim.models.Phrases:\n",
    "    return gensim.models.Phrases(tokenised_texts, **kwargs)\n",
    "\n",
    "\n",
    "def apply_ngram_model(ngram_model: gensim.models.Phrases, tokenised_texts: list[list[str]]) -> list[list[str]]:\n",
    "    return [ngram_model[tokens] for tokens in tokenised_texts]\n",
    "\n",
    "\n",
    "@curry(2)\n",
    "def filter_only_ngram_tokens(delimiter, tokenised_texts: list[list[str]]) -> list[list[str]]:\n",
    "    return [[token for token in tokens if delimiter in token] for tokens in tokenised_texts]\n",
    "\n",
    "\n",
    "@curry(2)\n",
    "def concat_ngram_tokens(delimiter, tokenised_texts: list[list[str]]) -> list[list[str]]:\n",
    "    return [[token.replace(delimiter, \"\") for token in tokens] for tokens in tokenised_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f6625f",
   "metadata": {},
   "source": [
    "create pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cce0c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_bigram = process_ngram(\n",
    "    train_ngram_model({\"min_count\": 1, \"threshold\": 0.1, \"delimiter\": NGRAM_DELIMITER}),\n",
    "    apply_ngram_model,\n",
    "    filter_only_ngram_tokens(NGRAM_DELIMITER),\n",
    "    concat_ngram_tokens(NGRAM_DELIMITER),\n",
    ")\n",
    "\n",
    "process_trigram = process_ngram(\n",
    "    train_ngram_model({\"min_count\": 1, \"threshold\": 0.1, \"delimiter\": NGRAM_DELIMITER}),\n",
    "    apply_ngram_model,\n",
    "    filter_only_ngram_tokens(NGRAM_DELIMITER),\n",
    "    concat_ngram_tokens(NGRAM_DELIMITER),\n",
    ")\n",
    "\n",
    "\n",
    "generate_ngram_pipeline = compose(\n",
    "    # filter long enough texts\n",
    "    filter_by_length(20),\n",
    "    tokenise,\n",
    "    filter_list_tokens_by_regex(r\"^5\"),\n",
    "    filter_list_tokens_by_regex(r\"^\\s+$\"),\n",
    "    # --- train ngram models below ---\n",
    "    prepare_data_for_ngram,\n",
    "    process_bigram,\n",
    "    process_trigram,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bedd3e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_ngram_pipeline(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9caa1346",
   "metadata": {},
   "outputs": [],
   "source": [
    "models, ngrams, ngrams_filtered = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e368463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['‡∏Å‡∏¥‡∏ô‡∏ô‡πâ‡∏≥‡∏ã‡∏∏‡∏õ', '‡∏ô‡∏∞‡∏≠‡∏£‡πà‡∏≠‡∏¢'],\n",
       " ['‡∏ô‡∏∞‡∏°‡∏∂‡∏á'],\n",
       " [],\n",
       " ['‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡πÅ‡∏°‡πà'],\n",
       " ['‡πÄ‡∏Ñ‡∏£', '‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà'],\n",
       " ['‡πÉ‡∏Ñ‡∏£‡∏ß‡πà‡∏≤', '‡∏à‡∏∞‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á'],\n",
       " ['‡∏ä‡πà‡∏ß‡∏á‡∏ô‡∏µ‡πâ‡∏à‡∏∞', '‡∏Å‡∏£‡∏≠‡∏ö‡πÜ'],\n",
       " ['‡∏ä‡πà‡∏ß‡∏¢‡πÜ', '‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡∏∞'],\n",
       " ['‡∏ä‡∏≠‡∏ö‡∏Å‡∏¥‡∏ô', '‡∏ä‡πâ‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡∏ö', '‡∏û‡∏µ‡πà‡∏ô‡πâ‡∏≥'],\n",
       " ['‡πÄ‡∏°‡∏ô‡∏π‡∏Ç‡∏≠‡∏á'],\n",
       " [],\n",
       " ['‡πÄ‡∏ã‡∏ô‡∏ó', '2‡∏°‡∏µ', '‡∏°‡∏±‡πâ‡∏¢‡∏Ñ‡πà‡∏∞'],\n",
       " [],\n",
       " ['‡πÄ‡πÄ‡∏î‡∏Å'],\n",
       " ['‡∏™‡πà‡∏á‡∏°‡∏≤'],\n",
       " ['‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÅ‡∏•‡πâ‡∏ß'],\n",
       " ['‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤', '‡∏Ñ‡πà‡∏≠‡∏¢‡πÑ‡∏õ'],\n",
       " ['‡πÅ‡∏•‡πâ‡∏ß‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì'],\n",
       " ['‡∏™‡∏∏‡∏î‡∏¢‡∏≠‡∏î‡πÄ‡∏•‡∏¢'],\n",
       " ['‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏±‡∏î', '‡∏™‡∏±‡∏Å‡πÉ‡∏ö'],\n",
       " ['‡πÅ‡∏•‡πâ‡∏ß‡∏ô‡∏∞'],\n",
       " ['‡∏à‡πã‡∏≤‡∏≤‡∏≤‡∏≤', '‡∏´‡∏¥‡∏ß‡∏ß‡∏ß‡∏ß'],\n",
       " ['‡∏™‡∏∏‡∏î‡πÜ'],\n",
       " ['‡πÅ‡∏ñ‡∏ß‡∏ô‡∏µ‡πâ', 'Hotpot', '‡∏´‡∏£‡∏≠‡∏≠'],\n",
       " [],\n",
       " ['‡∏à‡∏±‡∏á‡πÄ‡∏•‡∏¢', '‡∏Ñ‡πà‡∏≤‡∏≤‡∏≤'],\n",
       " ['‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÜ', '‡∏ï‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢'],\n",
       " ['‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£‡∏Ñ‡∏±‡∏ö'],\n",
       " ['‡∏û‡∏≠‡∏Ñ‡πà‡∏≤'],\n",
       " [],\n",
       " ['‡πÅ‡∏õ‡πâ‡∏á‡πÄ‡∏à‡πâ‡∏≤', '‡∏î‡∏µ‡∏á‡∏≤‡∏°‡∏Ñ‡πà‡∏∞'],\n",
       " ['‡πÑ‡∏õ‡πÜ‡πÜ‡πÜ'],\n",
       " ['‡πÑ‡∏õ‡πÜ‡πÜ‡πÜ‡πÜ‡πÜ', '‡πÄ‡∏î‡πá‡∏Å‡πÜ', '‡πÑ‡∏õ‡∏Å‡∏±‡∏ô'],\n",
       " ['‡πÑ‡∏õ‡∏Å‡∏¥‡∏ô', '‡∏°‡πâ‡∏≤‡∏¢'],\n",
       " ['‡πÑ‡∏õ‡∏Å‡∏¥‡∏ô‡∏Å‡∏±‡∏ô', '‡∏Ñ‡∏á‡∏≠‡∏¢‡∏≤‡∏Å'],\n",
       " ['‡∏à‡πâ‡∏≤‡∏≤‡∏≤'],\n",
       " ['‡∏≠‡∏∂‡∏î‡∏≠‡∏±‡∏î‡∏°‡∏≤‡∏Å'],\n",
       " ['‡πÑ‡∏õ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ', '‡πÑ‡∏î‡πâ‡∏°‡∏±‡πâ‡∏¢'],\n",
       " ['‡∏ú‡∏°‡∏Å‡∏¥‡∏ô', '‡πÄ‡∏ö‡∏µ‡∏¢‡∏£‡πå‡∏™‡∏¥‡∏á‡∏´‡πå'],\n",
       " [],\n",
       " ['‡∏ô‡∏µ‡πâ‡∏ä‡∏≠‡∏ö', 'MG3'],\n",
       " ['‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏•‡∏¢'],\n",
       " ['‡∏û‡∏ß‡∏Å‡∏û‡∏µ‡πà', '‡∏™‡∏∏‡∏î‡∏¢‡∏≠‡∏î‡∏Ñ‡∏±‡∏ö'],\n",
       " ['‡∏û‡∏≤‡πÑ‡∏õ', '‡∏à‡πà‡∏≤‡∏¢‡∏ï‡∏±‡∏á', '‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡∏∞'],\n",
       " ['‡∏°‡∏µ‡πÇ‡∏õ‡∏£'],\n",
       " ['‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏õ‡πà‡∏∞'],\n",
       " ['‡πÄ‡∏°‡∏ô‡∏π‡∏Æ‡∏¥‡∏ï', '‡∏ó‡∏µ‡πà‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô', '‡∏Ñ‡∏ô‡∏ä‡∏≠‡∏ö'],\n",
       " ['‡∏¢‡∏π‡πÄ‡∏ã', '‡∏≠‡∏£‡∏µ'],\n",
       " [],\n",
       " ['‡∏£‡∏µ‡πÇ‡∏ß‡πà'],\n",
       " ['‡∏£‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£'],\n",
       " ['‡∏£‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£'],\n",
       " ['‡∏£‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£', '‡∏ö‡∏∏‡∏ü‡πÄ‡∏ü‡πà'],\n",
       " ['‡πÄ‡∏£‡∏≤‡∏ä‡∏≠‡∏ö', '‡∏Å‡∏¥‡∏ô‡∏ô‡πâ‡∏≥‡∏™‡∏¥‡∏á‡∏´‡πå'],\n",
       " ['‡∏•‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß'],\n",
       " ['‡∏´‡∏•‡∏±‡∏á‡∏õ‡∏µ‡πÉ‡∏´‡∏°‡πà', '‡∏Å‡∏∞‡πÑ‡∏î‡πâ'],\n",
       " ['‡∏•‡∏π‡∏Å‡∏ä‡∏¥‡πâ‡∏ô‡∏Ñ‡∏£‡∏¥‡∏™‡∏ï‡∏±‡∏•'],\n",
       " ['‡πÄ‡∏•‡πà‡∏°‡∏ô‡∏µ‡πâ', '‡∏ô‡πà‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡∏°‡∏≤‡∏Å'],\n",
       " ['‡πÅ‡∏•‡πâ‡∏ß‡πÇ‡∏õ‡∏£', '‡∏≠‡∏∞‡πÑ‡∏£‡∏Ñ‡πà‡∏∞'],\n",
       " ['‡πÅ‡∏™‡∏á‡πÇ‡∏™‡∏°‡∏ô‡∏∞'],\n",
       " ['‡πÑ‡∏õ‡∏Å‡∏¥‡∏ô', '‡∏ö‡∏≤‡∏ö‡∏µ‡∏Å‡πâ‡∏≠‡∏ô'],\n",
       " ['‡∏†‡∏≤‡∏û‡∏™‡∏ß‡∏¢'],\n",
       " [],\n",
       " ['‡πÄ‡∏•‡∏¢‡∏¢‡∏¢'],\n",
       " ['‡∏™‡∏π‡πâ‡πÜ', '‡∏Ç‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô'],\n",
       " ['‡πÉ‡∏™‡πÜ', '‡∏°‡∏≤‡∏Å‡πÜ'],\n",
       " ['‡∏ô‡∏µ‡πâ‡∏ô‡∏∞'],\n",
       " ['‡∏î‡∏µ‡πä‡∏î‡∏µ'],\n",
       " [],\n",
       " [],\n",
       " ['‡∏Å‡∏¥‡∏ô‡∏≠‡∏µ‡∏Å‡πÅ‡∏•‡πâ‡∏ß'],\n",
       " [],\n",
       " ['‡πÑ‡∏õ‡πÅ‡∏ï‡πà'],\n",
       " ['‡πÑ‡∏´‡∏ô‡∏ö‡∏≠‡∏Å', '‡∏≠‡∏¢‡∏≤‡∏Å‡∏Å‡∏¥‡∏ô'],\n",
       " ['‡∏≠‡∏¢‡∏≤‡∏Å‡∏Å‡∏¥‡∏ô', '‡∏à‡∏±‡∏á‡πÄ‡∏•‡∏¢'],\n",
       " ['‡∏≠‡∏¢‡∏≤‡∏Å‡∏Å‡∏¥‡∏ô', '‡∏ö‡∏≤‡∏ö‡∏¥', '‡∏Ñ‡∏¥‡∏ß‡∏û‡∏•‡∏≤‡∏ã‡πà‡∏≤'],\n",
       " ['‡∏≠‡∏¢‡∏≤‡∏Å‡∏Å‡∏¥‡∏ô', '‡πÄ‡∏≠‡∏°‡πÄ‡∏Ñ', '‡∏Ñ‡∏Ñ‡∏Ñ‡∏Ñ', '‡∏Ñ‡∏Ñ‡∏Ñ‡∏Ñ'],\n",
       " ['‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞'],\n",
       " ['‡∏≠‡∏£‡πà‡∏≠‡∏¢‡∏°‡∏≤‡∏Å', '‡∏à‡∏¥‡∏á‡πÜ‡πÜ', '‡∏ï‡πâ‡∏≠‡∏á‡∏•‡∏≠‡∏á'],\n",
       " ['‡∏û‡∏≤‡πÄ‡∏Ñ‡πâ‡∏≤', '‡πÑ‡∏õ‡∏Å‡∏¥‡∏ô'],\n",
       " ['‡∏Ñ‡∏£‡∏±‡∏ö‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì'],\n",
       " ['‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏õ'],\n",
       " ['‡πÑ‡∏Æ‡πÄ‡∏ô‡πÄ‡∏Å‡πâ‡∏ô‡∏™‡∏±‡∏Å', '‡∏Ç‡∏ß‡∏î‡πÑ‡∏°‡πà'],\n",
       " ['‡πÄ‡∏Ñ‡∏¢‡∏ã‡πà‡∏≠‡∏°'],\n",
       " ['‡πÄ‡∏¢‡πâ‡πÜ‡πÜ'],\n",
       " ['‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ô‡∏µ‡πâ'],\n",
       " ['‡∏Å‡πá‡πÄ‡∏´‡πá‡∏ô', '‡∏¢‡∏ä‡∏≠‡∏ö'],\n",
       " ['‡∏Å‡∏¥‡∏ô‡πÑ‡∏Æ‡πÄ‡∏ô‡πÄ‡∏Å‡πâ‡∏ô'],\n",
       " ['‡πÅ‡∏Å‡∏£‡πà‡∏á‡∏à‡∏£‡∏¥‡∏á', '‡∏ï‡πâ‡∏≠‡∏á‡∏ô‡∏≤‡∏ß‡∏≤'],\n",
       " ['‡∏°‡∏≤‡∏Å‡∏Ñ‡πà‡∏∞'],\n",
       " ['‡∏°‡∏≤‡∏™‡∏î‡πâ‡∏≤2'],\n",
       " ['‡∏•‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢'],\n",
       " ['‡πÄ‡∏Ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å', '‡∏Å‡∏¥‡∏ô‡∏ö‡∏≤‡∏ö‡∏µ', '‡∏Å‡πâ‡∏≠‡∏ô‡∏ô‡∏ô'],\n",
       " [],\n",
       " [],\n",
       " ['‡πÑ‡∏õ‡∏Å‡∏¥‡∏ô‡∏Å‡∏±‡∏ô'],\n",
       " ['.‡∏≠‡∏¢‡∏≤‡∏Å'],\n",
       " ['‡∏à‡∏∞‡∏Å‡∏¥‡∏ô', '‡πÅ‡∏ï‡πà‡πÄ‡∏≠‡πá‡∏°'],\n",
       " ['‡∏à‡∏∞‡πÑ‡∏î‡πâ', '‡∏°‡∏±‡πâ‡∏¢‡∏•‡πà‡∏∞'],\n",
       " ['‡∏à‡∏∞‡πÑ‡∏õ', '‡πÉ‡∏ä‡πà‡∏°‡πà‡∏∞']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigram from the first 10 texts\n",
    "ngrams_filtered[2][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47831650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e90171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
