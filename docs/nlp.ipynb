{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6689a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pymonad.tools import curry\n",
    "\n",
    "import gensim\n",
    "\n",
    "\n",
    "from altr.nlp import compose, exclude_by_regex, prepare_data_for_ngram, process_ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a4fd9e",
   "metadata": {},
   "source": [
    "load example texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "064fca43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':3', '☺️', '🤤', '🤪', '😁', '😄', '😊', '😋', '😍', '😘']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"https://raw.githubusercontent.com/PyThaiNLP/wisesight-sentiment/refs/heads/master/pos.txt\")\n",
    "\n",
    "texts = [text.strip() for text in response.text.split(\"\\n\")]\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0186e8ce",
   "metadata": {},
   "source": [
    "define tokeniser function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8971a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(texts):\n",
    "    return [word_tokenize(text, engine=\"newmm\", keep_whitespace=False) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e2b25e",
   "metadata": {},
   "source": [
    "define some functions for text and token processing pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41253543",
   "metadata": {},
   "outputs": [],
   "source": [
    "@curry(2)\n",
    "def filter_by_length(length: int, texts: list[str]) -> list[str]:\n",
    "    return [text for text in texts if len(text) >= length]\n",
    "\n",
    "\n",
    "@curry(2)\n",
    "def exclude_tokens_by_regex(pattern: str, list_of_tokens: list[list[str]]) -> list[list[str]]:\n",
    "    return [exclude_by_regex(pattern)(tokens) for tokens in list_of_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667e560c",
   "metadata": {},
   "source": [
    "define functions for process ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b77b495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@curry(2)\n",
    "def train_ngram_model(kwargs: dict, tokenised_texts: list[list[str]]) -> gensim.models.Phrases:\n",
    "    return gensim.models.Phrases(tokenised_texts, **kwargs)\n",
    "\n",
    "\n",
    "def apply_ngram_model(ngram_model: gensim.models.Phrases, tokenised_texts: list[list[str]]) -> list[list[str]]:\n",
    "    return [ngram_model[tokens] for tokens in tokenised_texts]\n",
    "\n",
    "\n",
    "@curry(2)\n",
    "def filter_only_ngram_tokens(delimiter, tokenised_texts: list[list[str]]) -> list[list[str]]:\n",
    "    return [[token for token in tokens if delimiter in token] for tokens in tokenised_texts]\n",
    "\n",
    "\n",
    "@curry(2)\n",
    "def concat_ngram_tokens(delimiter, tokenised_texts: list[list[str]]) -> list[list[str]]:\n",
    "    return [[token.replace(delimiter, \"\") for token in tokens] for tokens in tokenised_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f6625f",
   "metadata": {},
   "source": [
    "create pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab6d38cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training ngram pipeline\n",
    "\n",
    "NGRAM_DELIMITER = \"<DELIM>\"\n",
    "\n",
    "process_bigram = process_ngram(\n",
    "    train_ngram_model({\"min_count\": 1, \"threshold\": 0.1, \"delimiter\": NGRAM_DELIMITER}),\n",
    "    apply_ngram_model,\n",
    "    filter_only_ngram_tokens(NGRAM_DELIMITER),\n",
    "    concat_ngram_tokens(NGRAM_DELIMITER),\n",
    ")\n",
    "\n",
    "process_trigram = process_ngram(\n",
    "    train_ngram_model({\"min_count\": 1, \"threshold\": 0.1, \"delimiter\": NGRAM_DELIMITER}),\n",
    "    apply_ngram_model,\n",
    "    filter_only_ngram_tokens(NGRAM_DELIMITER),\n",
    "    concat_ngram_tokens(NGRAM_DELIMITER),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cce0c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final text processing pipeline\n",
    "\n",
    "text_processing_pipeline = compose(\n",
    "    # filter long enough texts\n",
    "    filter_by_length(20),\n",
    "    tokenise,\n",
    "    exclude_tokens_by_regex(r\"^5\"),\n",
    "    exclude_tokens_by_regex(r\"^\\s+$\"),\n",
    "    # --- train ngram models below ---\n",
    "    prepare_data_for_ngram,\n",
    "    process_bigram,\n",
    "    process_trigram,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bedd3e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = text_processing_pipeline(texts)\n",
    "models, ngrams, ngrams_filtered = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e368463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['กินน้ำซุป', 'นะอร่อย'],\n",
       " ['นะมึง'],\n",
       " [],\n",
       " ['คิดถึงแม่'],\n",
       " ['เคร', 'ถ้าไม่'],\n",
       " ['ใครว่า', 'จะเลี้ยง'],\n",
       " ['ช่วงนี้จะ', 'กรอบๆ'],\n",
       " ['ช่วยๆ', 'ด้วยนะ'],\n",
       " ['ชอบกิน', 'ช้างครับ', 'พี่น้ำ'],\n",
       " ['เมนูของ']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigram from the first 10 texts\n",
    "ngrams_filtered[2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c1fdd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['แถวนี้มี'],\n",
       " ['ค่าาา^^'],\n",
       " ['แป้งเจ้านาง'],\n",
       " ['วันนี้ป่ะล่ะ'],\n",
       " ['ยูเซอรี'],\n",
       " ['ไปกินบาบีก้อน'],\n",
       " ['สู้ๆคับ'],\n",
       " ['อยากกินบาบิ'],\n",
       " ['อยากกินเอมเค'],\n",
       " ['ไปกินหน่อย']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trigram from the first 10 texts\n",
    "list(filter(lambda tokens: len(tokens) > 0, ngrams_filtered[3]))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a98c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
